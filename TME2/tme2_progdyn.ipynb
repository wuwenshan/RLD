{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color='red'> TME 2 - Programmation Dynamique </font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TME a pour objectif d'expérimenter les modèles algorithmes de programmation dynamique sur un MDP classique de type GridWorld. <br>\n",
    "Nous utiliserons au cours des TME de RL la plateforme en python gym (de open-ai ). <br>\n",
    "Pour l'installer, tapez la commande: <font color=\"green\"> pip3 install gym --proxy proxy:3128 --user </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gym et GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement gridworld-v0 pour gym (fourni dans le \u001c",
    "chier associé au TME) est une tâche de RL où un agent (point bleu) doit récolter des éléments jaunes dans un labyrinthe 2D et terminer sur une case verte, tout en évitant les cases roses (non terminales) et rouges (terminales). Ci-dessous quelques fonctions utiles pour les environnements :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> initialiser un environnement : env = gym.make('gridworld-v0') puis env.reset() </li>\n",
    "    <li> env.action_space permet de connaître l'ensemble des actions possibles; </li>\n",
    "    <li> obs,reward,done,info = env.step(action) permet de jouer l'action passée en argument : obs contient le nouvel état, reward la récompense associée à l'action, done un booléen pour savoir si le jeu est fini; </li>\n",
    "    <li> env.render() permet de visualiser le jeu (si env.verbose est à true); </li>\n",
    "</ul>\n",
    "\n",
    "Pour le cas du gridworld la fonction env.getMDP() retourne un couple (states,P), où:\n",
    "\n",
    "<ul>\n",
    "    <li> states est un dictionnaire associant les observations à des numéros d'états (states[gridworld.GridworldEnv.state2str(observation)] contient le numéro d'état d'une observation obs avec observation un tableau numpy représentant la carte); </li>\n",
    "    <li> P correspond à l'automate du MDP de la tâche : chaque clé est un état, chaque valeur un dictionnaire; pour ce 2ème dictionnaire, chaque clé est une action et la valeur une liste de tuples correspondant à la transition associée (proba de la transition,état destination,reward,done), où done est vrai si l'état de destination est un état terminal. Ainsi, P[state][action] permet de connaître\n",
    "pour un état et une action la liste des états atteignables avec leur probabilité et la récompense associée. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger sur le site le fichier source associé au TME. Le répertoire gridworld contient l'environnement du labyrinthe; le fichier randomAgent.py présente des exemples d'utilisation de l'environnement et de gym et en particulier un agent aléatoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'environnement gridworld il est possible de charger différentes cartes de problème par la fonction setPlan(\"gridworldPlan/planX.txt\",{0:-0.001,3:1,4:1,5:-1,6:-1}) prenant en argument le fichier de la carte à charger et une liste des récompenses associées aux différents types de cases du jeu : 0 correspond à une case vide, 1 correspond à un mur (pas de reward associé car impossible de s'y déplacer), 2 correspond au joueur, 3 correspond à une case verte, 4 une case jaune, 5 une case rouge et 6 une case rose. <br>\n",
    "Dans le module fourni, vous trouverez différentes cartes de jeu (X de 0 à 10), de difficulté variable, que vous devrez tester au cours du TME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Travail demandé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codez les algorithmes de policy iteration et de value iteration vus en cours et testez les sur l'environnement. Discutez des résultats obtenus sur les différentes cartes/configurations (vous pouvez aussi en imaginer d'autres). Vous pouvez aussi faire varier le paramètre de discount pour en observer l'impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "import pandas as pd\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration():\n",
    "    \n",
    "    def __init__(self, action_space, statedic, mdp, eps=1e-10, gamma=0.99):\n",
    "        self.statedic = statedic\n",
    "        self.mdp = mdp\n",
    "        self.action_space = action_space\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def act(self, obs, reward, done):\n",
    "        nb_s = len(self.statedic) # Nombre d'états\n",
    "        \n",
    "        # dict avec comme clé les différents états et comme valeur les récompenses initialisées aléatoirement\n",
    "        # V = dict(zip(self.statedic, np.random.random(nb_s)))\n",
    "\n",
    "        # tab de tab des états possibles\n",
    "        states = [s for s in self.statedic]\n",
    "        \n",
    "        # dict des états associés initialement à la valeur None\n",
    "        pi = {s: np.random.randint(0, self.action_space.n) for s in states}\n",
    "        V = {s: 0 for s in states}\n",
    "\n",
    "        ft = True # First Time\n",
    "        ft2 = True\n",
    "\n",
    "        while ft:\n",
    "            pi_copy = copy.deepcopy(pi)\n",
    "            while ft2:\n",
    "                V_copy = copy.deepcopy(V)\n",
    "                \n",
    "#                 for state, dico in self.mdp.items(): # parcours de tous les états\n",
    "#                     V_inter = 0\n",
    "#                     action = pi[state]\n",
    "                    \n",
    "                    V_inter = [np.sum([p*(rew+self.gamma*V_copy[s_dest]) \n",
    "                        for p, s_dest, rew, _ in self.mdp[state][action]])\n",
    "                              for action, tuples in dico.items()] \n",
    "                    \n",
    "#                     for p, s_dest, rew, _ in dico[action]:\n",
    "#                         V_inter += ( p * (rew + self.gamma * V_copy[s_dest]) )\n",
    "#                     #print(\"V inter : \", V_inter)\n",
    "#                     V[state] = V_inter\n",
    "\n",
    "                for s, dico in self.mdp.items():\n",
    "\n",
    "                    action = pi[s]\n",
    "                    V[s] = np.sum([p * (rew + self.gamma * V_copy[s_dest] ) for p, s_dest, rew, _ in self.mdp[s][action] ])\n",
    "\n",
    "#                 if ( np.linalg.norm(np.array(list(V_copy.values())) - np.array(list(V.values())))) <= self.eps:\n",
    "#                     ft2 = False\n",
    "\n",
    "                if np.sum(np.fabs(np.array(list(V_copy.values())) - np.array(list(V.values())))) <= self.eps:\n",
    "                    ft2 = False\n",
    "\n",
    "            \n",
    "            for state, dico in self.mdp.items(): # parcours de tous les états\n",
    "                \n",
    "                V_inter = [np.sum([p*(rew+self.gamma*V[s_dest]) \n",
    "                            for p, s_dest, rew, _ in tuples]) \n",
    "                                for action, tuples in dico.items() ]\n",
    "\n",
    "                #print(\"ll : \", len(V_inter))\n",
    "                pi[state] = np.argmax(V_inter)\n",
    "                \n",
    "                \n",
    "            if pi_copy == pi:\n",
    "                ft = False\n",
    "                \n",
    "\n",
    "            \n",
    "        return pi[gridworld.GridworldEnv.state2str(obs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration():\n",
    "    \n",
    "    def __init__(self, action_space, statedic, mdp, eps=0.001, gamma=0.99):\n",
    "        self.statedic = statedic\n",
    "        self.mdp = mdp\n",
    "        self.action_space = action_space\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def act(self, obs, reward, done):\n",
    "        nb_s = len(self.statedic) # Nombre d'états\n",
    "        V = dict(zip(self.statedic, np.random.random(nb_s)))\n",
    "        states = [s for s in self.statedic]\n",
    "        pi = {s: None for s in states}\n",
    "        ft = True # First Time\n",
    "        i = 0\n",
    "    \n",
    "        while ft:\n",
    "            V_copy = copy.deepcopy(V)\n",
    "            for state, dico in self.mdp.items(): # parcours de tous les états\n",
    "                \n",
    "                V_inter = [np.sum([p*(rew+self.gamma*V_copy[s_dest]) \n",
    "                        for p, s_dest, rew, _ in tuples]) \n",
    "                            for action, tuples in dico.items() ]\n",
    "                \n",
    "                V[state] = np.max(V_inter)\n",
    "                \n",
    "            if np.linalg.norm(np.array(list(V.values())) - np.array(list(V_copy.values()))) <= self.eps:\n",
    "                ft = False\n",
    "    \n",
    "        for state, dico in self.mdp.items(): # parcours de tous les états\n",
    "                \n",
    "            V_inter = [np.sum([p*(rew+self.gamma*V[s_dest]) \n",
    "                        for p, s_dest, rew, _ in self.mdp[state][action]]) \n",
    "                            for action, tuples in dico.items() ]\n",
    "\n",
    "            pi[state] = np.argmax(V_inter)\n",
    "    \n",
    "   \n",
    "            \n",
    "        return pi[gridworld.GridworldEnv.state2str(obs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agentType=\"random\", worldNumber=0):\n",
    "    # env.action_space : ens des actions possibles\n",
    "    # env.action_space.n : nombre d'actions possibles\n",
    "    # env.observation_space : ens des états possibles\n",
    "    # env.observation_space : nombre d'états possibles\n",
    "\n",
    "    env = gym.make(\"gridworld-v0\") # Init un environnement\n",
    "\n",
    "    # setPlan(arg1, arg2)\n",
    "    # arg1 : fichier de la carte à charger\n",
    "    # arg2 : liste de récompenses associées aux différents types de cases du jeu \n",
    "    env.setPlan(\"gridworldPlans/plan\"+str(worldNumber)+\".txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1})\n",
    "\n",
    "    env.verbose = True\n",
    "\n",
    "    statedic, mdp = env.getMDP()  \n",
    "\n",
    "    if agentType == \"random\":\n",
    "        agent = RandomAgent(env.action_space)\n",
    "        \n",
    "    elif agentType == \"policy\":\n",
    "        agent = PolicyIteration2(env.action_space, statedic, mdp)\n",
    "        \n",
    "    elif agentType == \"value\":\n",
    "        agent = ValueIteration(env.action_space, statedic, mdp)\n",
    "    \n",
    "    else:\n",
    "        agent = RandomAgent(env.action_space)\n",
    "        print(\"Agent inconnu : agent random par défaut\")\n",
    "    \n",
    "\n",
    "    # Faire un fichier de log sur plusieurs scenarios\n",
    "    outdir = 'gridworld-v0/random-agent-results'\n",
    "    envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "\n",
    "\n",
    "    countRewards = []\n",
    "\n",
    "    episode_count = 1000\n",
    "    reward = 0\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    FPS = 0.001\n",
    "\n",
    "    for i in tqdm(range(episode_count)):\n",
    "\n",
    "        obs = envm.reset()\n",
    "        env.verbose = (i % 100 == 0 and i > 0)  # afficher 1 episode sur 100\n",
    "#         if env.verbose:\n",
    "#             env.render(FPS)\n",
    "#             env.render(mode=\"human\")\n",
    "        j = 0\n",
    "        rsum = 0\n",
    "        while True:\n",
    "            action = agent.act(obs, reward, done)\n",
    "            obs, reward, done, _ = envm.step(action)\n",
    "            rsum += reward\n",
    "            j += 1\n",
    "#             if env.verbose:\n",
    "#                 env.render(FPS)\n",
    "            if done:\n",
    "                #print(\"Episode : \" + str(i) + \" rsum=\" + str(rsum) + \", \" + str(j) + \" actions\")\n",
    "                countRewards.append(rsum)\n",
    "                break\n",
    "\n",
    "    np.save(\"rewards_gridworld_\"+str(worldNumber)+\"_\"+agentType+\".npy\", countRewards)          \n",
    "    print(\"Mean & std : \", np.mean(countRewards), np.std(countRewards))\n",
    "    print(\"done\")\n",
    "    env.close()\n",
    "    \n",
    "    return countRewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Quelques résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 667.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean & std :  -1.096846 0.10000114141348596\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cr = play(\"policy\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5183289999999999"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load(\"rewards_gridworld_0_policy.npy\")\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 23, 'd': 27}\n",
      "{'s': 23, 'd': 37}\n"
     ]
    }
   ],
   "source": [
    "a = {'s' : 23, 'd' : 37}\n",
    "b = {'s' : 23, 'd' : 27}\n",
    "print(b)\n",
    "b = copy.deepcopy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "a = 12\n",
    "print(np.sum(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> Le temps de convergence de l'algorithme **Value Iteration** est plus rapide que celle de **Policy Iteration** </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
